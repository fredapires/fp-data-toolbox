{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **archive_deck_list**\n",
    "---\n",
    "\n",
    "<br><br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Variable Inputs**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ====================================\n",
    "# input directory\n",
    "input_dir_paper_decks = 'C:\\\\git\\\\mtg-proj\\\\data\\\\input\\\\helvault_csv'\n",
    "input_dir_digital_decks = 'C:\\\\git\\\\mtg-proj\\\\data\\\\input\\\\moxfield_txt\\\\*'\n",
    "input_deck_prefix = 'deck-' # USER INPUT - prefix string for file to be picked up by logic below\n",
    "\n",
    "### ====================================\n",
    "# output\n",
    "output_dir_paper = 'C:\\\\git\\\\mtg-proj\\\\data\\\\output\\\\deck_lists\\\\paper'\n",
    "output_dir_digital = 'C:\\\\git\\\\mtg-proj\\\\data\\\\output\\\\deck_lists\\\\digital'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Objectives**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Take deck lists in the .txt format from a specified directory and convert them to .csv files with 2 fields: quantity and name.\n",
    "    -   In the Scryfall name format for MDFCs\n",
    "    -   update paper decklist (active and archive)\n",
    "        -   with logic for comparing formatted deck lists (as pandas dfs)\n",
    "\n",
    "<br>\n",
    "\n",
    "-   Inputs:\n",
    "    -   Decklists from moxfield.com\n",
    "        -   in the MTGO format of\n",
    "            -   1 Black Ritual\n",
    "            -   4 Forrest\n",
    "            -   etc.\n",
    "\n",
    "<br>\n",
    "\n",
    "-   Outputs:\n",
    "    -   deck lists in .csv format\n",
    "        -   2 fields: quantity and name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualizing ETL pipeline (example)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # graphviz data pipeline example...\n",
    "# import graphviz\n",
    "\n",
    "# # Name of file and comment as arguments\n",
    "# graphviz_file_name  = 'Example Pipeline'\n",
    "# graphviz_comment    = 'Example Pipeline'\n",
    "# pipeline_graph = graphviz.Digraph(graphviz_file_name, comment = graphviz_comment)\n",
    "\n",
    "# # Definning alias and display name of each node\n",
    "# pipeline_graph.node('1', '1 - Input')\n",
    "# pipeline_graph.node('2', '2 - Cleaning')\n",
    "# pipeline_graph.node('3', '3 - Formatting')\n",
    "# pipeline_graph.node('4', '4 - Processing')\n",
    "# pipeline_graph.node('5', '5 - Analysis')\n",
    "# pipeline_graph.node('6', '6 - Outputs')\n",
    "\n",
    "# # Defining connections between nodes\n",
    "# pipeline_graph.edges(['12', '23', '34', '45'])\n",
    "# pipeline_graph.edge('1', '6', constraint='false')\n",
    "\n",
    "# # save file to path\n",
    "# #pipeline_graph.render(directory='graphviz-output')\n",
    "\n",
    "# # printrender of graph\n",
    "# pipeline_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imports / Environment Setup**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports and Settings**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# import project_path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fp_data_toolbox import eda, notifier\n",
    "from datetime import date\n",
    "import time\n",
    "\n",
    "ts = time.time()\n",
    "notifier.setup() # Enable for windows toast notifications on Jupyter cell complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Variables**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_dt = eda.get_curr_dt()\n",
    "curr_dt = str(date.today())\n",
    "curr_dt\n",
    "\n",
    "csv_suffix=\".csv\"\n",
    "txt_suffix=\".txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ingestion/Preprocessing**\n",
    "\n",
    "---\n",
    "\n",
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paper deck list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".gitkeep\n",
      "bndr-main-Commanders\n",
      "bndr-main-Mana Base\n",
      "bndr-main-Unique & Misc\n",
      "box-Bulk Legit Staples\n",
      "box-Bulk Legit\n",
      "box-Bulk Proxy\n",
      "deck-Anowon\n",
      "input.csv detected: C:\\git\\mtg-proj\\helvault_csv_inputs\\deck-Anowon.csv\n",
      "input == active; escaping this iteration\n",
      "deck-Beginner Yangling\n",
      "input.csv detected: C:\\git\\mtg-proj\\helvault_csv_inputs\\deck-Beginner Yangling.csv\n",
      "input == active; escaping this iteration\n",
      "deck-Breena\n",
      "input.csv detected: C:\\git\\mtg-proj\\helvault_csv_inputs\\deck-Breena.csv\n",
      "input == active; escaping this iteration\n",
      "deck-Galea\n",
      "input.csv detected: C:\\git\\mtg-proj\\helvault_csv_inputs\\deck-Galea.csv\n",
      "input == active; escaping this iteration\n",
      "deck-Nghathrod\n",
      "input.csv detected: C:\\git\\mtg-proj\\helvault_csv_inputs\\deck-Nghathrod.csv\n",
      "input == active; escaping this iteration\n",
      "deck-Obuun\n",
      "input.csv detected: C:\\git\\mtg-proj\\helvault_csv_inputs\\deck-Obuun.csv\n",
      "input == active; escaping this iteration\n",
      "deck-Prosper\n",
      "input.csv detected: C:\\git\\mtg-proj\\helvault_csv_inputs\\deck-Prosper.csv\n",
      "input == active; escaping this iteration\n",
      "deck-Ranar\n",
      "input.csv detected: C:\\git\\mtg-proj\\helvault_csv_inputs\\deck-Ranar.csv\n",
      "input == active; escaping this iteration\n",
      "deck-Rin and Seri\n",
      "input.csv detected: C:\\git\\mtg-proj\\helvault_csv_inputs\\deck-Rin and Seri.csv\n",
      "input == active; escaping this iteration\n",
      "deck-Rough and Tumble Arena 2021\n",
      "input.csv detected: C:\\git\\mtg-proj\\helvault_csv_inputs\\deck-Rough and Tumble Arena 2021.csv\n",
      "input == active; escaping this iteration\n",
      "deck-Sefris\n",
      "input.csv detected: C:\\git\\mtg-proj\\helvault_csv_inputs\\deck-Sefris.csv\n",
      "input == active; escaping this iteration\n",
      "deck-Sneak Attack Arena 2021\n",
      "input.csv detected: C:\\git\\mtg-proj\\helvault_csv_inputs\\deck-Sneak Attack Arena 2021.csv\n",
      "input == active; escaping this iteration\n",
      "deck-Wyleth\n",
      "input.csv detected: C:\\git\\mtg-proj\\helvault_csv_inputs\\deck-Wyleth.csv\n",
      "input == active; escaping this iteration\n"
     ]
    }
   ],
   "source": [
    "### [x] consume the file names from the input directory\n",
    "\n",
    "### TODO [ ] def sort and reindex colums as a function\n",
    "\n",
    "### ====================================\n",
    "directory = input_dir_paper_decks\n",
    "for filename in os.scandir(directory):\n",
    "    if filename.is_file():\n",
    "        ### [x] pull just the file name; i.e.: 'deck-Anowon-v1.00.csv'\n",
    "        filename_str=filename.path.rsplit('\\\\', 1)[1] or filename.path \n",
    "        filename_str=filename_str.rsplit('.', 1)[0] or filename_str # find the filename \n",
    "        \n",
    "        if filename.path.endswith(csv_suffix):\n",
    "            if filename_str.startswith(input_deck_prefix):\n",
    "                \n",
    "                print('input.csv detected: '+filename.path)\n",
    "                \n",
    "                in_dir_stg=filename.path.rsplit('\\\\',2)[0] # [x] split this into the correct path string\n",
    "                in_file_nm=filename.path.rsplit('\\\\',2)[2] # [x] split this into the correct path string\n",
    "                deck_nm=filename.path.rsplit('\\\\',2)[2].rsplit(input_deck_prefix,2)[1].rsplit(csv_suffix,2)[0] # [x] split this into the correct path string\n",
    "                \n",
    "                ### [x] change the below naming convention to match the one setup in C:\\git\\mtg-proj\\helvault_csv_inputs\n",
    "                output_dir_paper_stg = output_dir_paper+'\\\\'+deck_nm\n",
    "                output_path_paper = output_dir_paper_stg+'\\\\'+'deck-'+deck_nm+'.csv'\n",
    "                arch_dir_paper = output_dir_paper+'\\\\'+deck_nm+'\\\\'+'.archive'+'\\\\'\n",
    "                arch_path_paper = arch_dir_paper+'deck-'+deck_nm+'.'+curr_dt+'.csv'\n",
    "                \n",
    "                ### ingest .csv as df here\n",
    "                df_stg=pd.read_csv(filename.path)\n",
    "                # print(df_stg.head())\n",
    "                \n",
    "                ### ====================================\n",
    "                ### [x] Data cleaning operations here\n",
    "                \n",
    "                df_stg=df_stg.sort_values(by=['name','quantity'],ignore_index=True)\n",
    "                df=df_stg.reindex([\n",
    "                    'quantity',\n",
    "                    'name'\n",
    "                ], axis=1)\n",
    "                df=df.rename(columns={\"quantity\": \"count\", \"name\": \"name\"})\n",
    "                ### [x] df should just be count and name Aafter this\n",
    "                \n",
    "                ### ====================================\n",
    "                ### [x] check whether current file out already exists\n",
    "                path = Path(output_path_paper)\n",
    "                if path.is_file():\n",
    "                    # print(f'The file {output_path_paper} exists')\n",
    "                    \n",
    "                    df_existing = pd.read_csv(output_path_paper, header=None)\n",
    "                    df_existing = df_existing.rename(columns={0: \"count\", 1: \"name\"})\n",
    "                    \n",
    "                    ### [x] re-sort dfs by name, and reindex before comparing \n",
    "                    df_existing.sort_values(by=['name','count'],ignore_index=True)\n",
    "                    df_existing=df_existing.reindex([\n",
    "                        'count',\n",
    "                        'name'\n",
    "                    ], axis=1)\n",
    "\n",
    "                    df.sort_values(by=['name'])\n",
    "                    df=df.reindex([\n",
    "                        'count',\n",
    "                        'name'\n",
    "                    ], axis=1)\n",
    "                    \n",
    "                    ### [x] merge dfs\n",
    "                    df_merge = pd.merge(df, df_existing, on=['name'], how='outer')\n",
    "                    # cleaning\n",
    "                    df_merge['count_x'] = df_merge['count_x'].fillna(0)\n",
    "                    df_merge['count_y'] = df_merge['count_y'].fillna(0)\n",
    "                    df_merge = df_merge.rename(columns={\n",
    "                        \"count_x\": \"count_src\", \n",
    "                        \"count_y\": \"count_tgt\"\n",
    "                        })\n",
    "                    \n",
    "                    df_merge = df_merge.reindex([ # not sure if this is necessary\n",
    "                        'name',\n",
    "                        'count_src',\n",
    "                        'count_tgt'\n",
    "                    ], axis=1)\n",
    "                    \n",
    "                    # print(df_merge)\n",
    "                    \n",
    "                    ### [x] write logic for returning a boolean\n",
    "                        ### [x] testing completed\n",
    "                    countSrc=df_merge['count_src']\n",
    "                    countTgt=df_merge['count_tgt']\n",
    "                    \n",
    "                    df_equals_bool = 0 in countSrc or 0 in countTgt\n",
    "                    \n",
    "                    if df_equals_bool: ### [x] if the two dfs equal eachother, escape operation\n",
    "                        print(\"input == active; escaping this iteration\")\n",
    "                        continue\n",
    "                    print(\"input <> active; writing new active and archive versions\")\n",
    "                    \n",
    "                    ### [ ] setup path variables for differences df outputs\n",
    "                    # changes_dir = ''\n",
    "                    # changes_in_path = changes_dir+'\\\\'+''\n",
    "                    # changes_out_path = changes_dir+'\\\\'+''\n",
    "                    \n",
    "                    ### [ ] logic for comparing df_merge and returning only the differences\n",
    "                        ### in 2 separate dfs (in changes and out changes)\n",
    "                    \n",
    "                    # df_merge\n",
    "                    \n",
    "                    ### [ ] return input and output differences in separate dfs\n",
    "                    \n",
    "                    ### outputs\n",
    "                        ### [x] main deck list record (output_path_paper)\n",
    "                        ### [x] archive version (arch_path_paper)\n",
    "                        ### [ ] in / out lists of differences between DFs (df_merge)\n",
    "                    \n",
    "                    # df.to_csv(changes_in_path,index=False)\n",
    "                    # df.to_csv(changes_out_path,index=False)\n",
    "                    \n",
    "                    df.to_csv(output_path_paper,index=False)\n",
    "                    df.to_csv(arch_path_paper,index=False)\n",
    "                    ### adapt the belowus\n",
    "                    ### [ ] moxfield output formatting (.txt with just name and count)\n",
    "                    # df.to_csv(r'c:\\data\\pandas.txt', header=None, index=None, sep=' ', mode='a')\n",
    "                \n",
    "                else:\n",
    "                    print(f'File **{output_path_paper}** does not exist')\n",
    "                    \n",
    "                    # print(output_path_paper)\n",
    "                    # print(arch_path_paper)\n",
    "                    \n",
    "                    ### Create target directory & all intermediate directories if don't exists\n",
    "                    if not os.path.exists(output_dir_paper):\n",
    "                        os.makedirs(output_dir_paper)\n",
    "                        print(\"Directory \" , output_dir_paper ,  \" Created \")\n",
    "                    else:\n",
    "                        print(\"Directory \" , output_dir_paper ,  \" already exists\")\n",
    "\n",
    "                    if not os.path.exists(arch_dir_paper):\n",
    "                        os.makedirs(arch_dir_paper)\n",
    "                        print(\"Directory \" , arch_dir_paper ,  \" Created \")\n",
    "                    else:\n",
    "                        print(\"Directory \" , arch_dir_paper ,  \" already exists\")\n",
    "\n",
    "                    ### output\n",
    "                    df.to_csv(output_path_paper,index=False)\n",
    "                    df.to_csv(arch_path_paper,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Digital deck list ingestion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt search hit\n",
      "File C:\\git\\mtg-proj\\deck_lists\\digital\\Pako\\deck-Pako.csv exists\n",
      "input == active; escaping this iteration\n"
     ]
    }
   ],
   "source": [
    "### [ ] update the below with new fields to be created on the output\n",
    "\n",
    "### example of find .csv file names logic\n",
    "# def find_csv_filenames( path_to_dir, suffix=\".csv\" ):\n",
    "#     filenames = listdir(path_to_dir)\n",
    "#     return [ filename for filename in filenames if filename.endswith( suffix ) ]\n",
    "\n",
    "directory = input_dir_digital_decks\n",
    "### ====================================\n",
    "### looping + iterating on 'directory' for all deck list .txt files\n",
    "listing = glob.glob(directory)\n",
    "for foldername in listing:\n",
    "    for filename in os.scandir(foldername):\n",
    "        if filename.is_file():\n",
    "            if filename.path.endswith('.txt'):\n",
    "                print('txt search hit')                \n",
    "                # ====================================\n",
    "                filename_str = str(filename)\n",
    "                deck_nm=filename.path.rsplit('\\\\',2)[1]\n",
    "                \n",
    "                output_dir_df=output_dir_digital+'\\\\'+deck_nm\n",
    "                arch_dir_df=output_dir_df+'\\\\'+'.archive'\n",
    "\n",
    "                ### [ ] change the below naming convention to match the one setup in C:\\git\\mtg-proj\\helvault_csv_inputs\n",
    "                output_path_df=output_dir_df+'\\\\'+'deck-'+deck_nm+'.csv'\n",
    "                arch_path_df=arch_dir_df+'\\\\'+'deck-'+deck_nm+'-'+curr_dt+'.csv'\n",
    "                ### ====================================\n",
    "                # print('Deck list name: '+deck_nm)\n",
    "                # print(output_path_df)\n",
    "                # print(arch_path_df)\n",
    "                ### ====================================\n",
    "\n",
    "                # ### read in a text file with 'read_csv' below\n",
    "                df = pd.read_csv(filename.path, sep='\\t',header =None,names=['count_nm'])\n",
    "\n",
    "                # ### Data cleaning operations here\n",
    "                count_nm_s = pd.Series(df['count_nm'], index=df.index)\n",
    "                split_df = count_nm_s.str.split(\" \", expand=True, n=1) # split series on first space only and return as a df\n",
    "                df_stg = split_df\n",
    "                df_stg[\"count\"]= split_df[0]\n",
    "                df_stg[\"name\"]= split_df[1]\n",
    "                df = df_stg.drop(columns=[0,1])\n",
    "                \n",
    "                ### ====================================\n",
    "\n",
    "                path = Path(output_path_df)\n",
    "                if path.is_file():\n",
    "                    print(f'File {output_path_df} exists')\n",
    "                    \n",
    "                    df_existing = pd.read_csv(output_path_df, header=None)\n",
    "                    df_existing = df_existing.rename(columns={0: \"count\", 1: \"name\"})\n",
    "                    \n",
    "                    ### [x] re-sort dfs by name, and reindex before comparing \n",
    "                    df_existing.sort_values(by=['name'])\n",
    "                    df_existing=df_existing.reindex([\n",
    "                        'count',\n",
    "                        'name'\n",
    "                    ], axis=1)\n",
    "\n",
    "                    df.sort_values(by=['name'])\n",
    "                    df=df.reindex([\n",
    "                        'count',\n",
    "                        'name'\n",
    "                    ], axis=1)\n",
    "                    \n",
    "                    ### [x] merge dfs\n",
    "                    df_merge = pd.merge(df, df_existing, on=['name'], how='outer')\n",
    "                    # cleaning\n",
    "                    df_merge['count_x'] = df_merge['count_x'].fillna(0)\n",
    "                    df_merge['count_y'] = df_merge['count_y'].fillna(0)\n",
    "                    df_merge = df_merge.rename(columns={\n",
    "                        \"count_x\": \"count_src\", \n",
    "                        \"count_y\": \"count_tgt\"\n",
    "                        })\n",
    "                    \n",
    "                    df_merge = df_merge.reindex([\n",
    "                        'name',\n",
    "                        'count_src',\n",
    "                        'count_tgt'\n",
    "                    ], axis=1)\n",
    "                    \n",
    "                    ### [x] write logic for returning a boolean\n",
    "                        ### [x] testing completed\n",
    "                    countSrc=df_merge['count_src']\n",
    "                    countTgt=df_merge['count_tgt']\n",
    "                    \n",
    "                    df_equals_bool = 0 in countSrc or 0 in countTgt\n",
    "                    \n",
    "                    if df_equals_bool: ### [x] if the two dfs equal eachother, escape operation\n",
    "                        print(\"input == active; escaping this iteration\")\n",
    "                        continue\n",
    "                    print(\"input <> active; writing new active and archive versions\")\n",
    "\n",
    "                    ### [ ] return input and output differences in separate dfs\n",
    "\n",
    "                    ### outputs\n",
    "                        ### [x] main deck list record (output_path_paper)\n",
    "                        ### [x] archive version (arch_path_paper)\n",
    "                        ### [ ] in / out lists of differences between DFs (df_merge)\n",
    "\n",
    "                    df.to_csv(output_path_df,index=False)\n",
    "                    df.to_csv(arch_path_df,index=False)\n",
    "                    \n",
    "                else:\n",
    "                    print(f'File {output_path_df} does not exist')\n",
    "                    \n",
    "                    ### Create target directory & all intermediate directories if don't exists\n",
    "                    if not os.path.exists(output_dir_df):\n",
    "                        os.makedirs(output_dir_df)\n",
    "                        print(\"Directory \" , output_dir_df ,  \" Created \")\n",
    "                    else:\n",
    "                        print(\"Directory \" , output_dir_df ,  \" already exists\")\n",
    "\n",
    "                    if not os.path.exists(arch_dir_df):\n",
    "                        os.makedirs(arch_dir_df)\n",
    "                        print(\"Directory \" , arch_dir_df ,  \" Created \")\n",
    "                    else:\n",
    "                        print(\"Directory \" , arch_dir_df ,  \" already exists\")\n",
    "\n",
    "                    ### output\n",
    "                    df.to_csv(output_path_df,index=False)\n",
    "                    df.to_csv(arch_path_df,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "bf2256466664d08ad5829690b78a52ff85021f5c7c81ebd85ac567841dd5c95f"
   }
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
