{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **xgboost_timeseries_modeling_template**\n",
    "\n",
    "---\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Variable Inputs**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = ''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Environment Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mxgboost\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mxgb\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m mean_squared_error\n\u001b[0;32m      7\u001b[0m \u001b[39m# custom imports\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m# general eda functions and win toast notifier on cell completion\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfp_data_toolbox\u001b[39;00m \u001b[39mimport\u001b[39;00m eda, notifier, environment\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# custom imports\n",
    "# general eda functions and win toast notifier on cell completion\n",
    "from fp_data_toolbox import eda, notifier, environment\n",
    "# Magics env settings...\n",
    "# Setup sqlalchemy connection url for MSSQL Server connection here\n",
    "%matplotlib inline\n",
    "%load_ext google.cloud.bigquery\n",
    "%load_ext sql\n",
    "%env DATABASE_URL = mssql+pyodbc: // SCFDW2/scfdw_core?driver = SQL+Server+Native+Client+11.0\n",
    "%config SqlMagic.autocommit = True\n",
    "%config SqlMagic.autopandas = True\n",
    "# env setup functions\n",
    "notifier.setup()  # Enable for windows toast notifications on Jupyter cell complete\n",
    "# Enable to setup a ydata_profiling config.yaml file in the parent project\n",
    "yaml_config_path = environment.ydata_yaml_setup()\n",
    "# env variables\n",
    "gbq_project_id = 'analytics-scfinance-thd'  # USER INPUT\n",
    "sql_conn = 'mssql+pyodbc://SCFDW2/scfdw_core?driver=SQL+Server+Native+Client+11.0'  # USER INPUT\n",
    "df = pd.DataFrame()    # creating empty dataframe variable\n",
    "params = {}            # creating empty parameters dictionary\n",
    "# params = fcal.pull_fin_cal_temp_var()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Objectives**\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example of XGBoost ML timeseries prediction pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualizing ETL pipeline (example)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg  width=\"330\" height=\"55\"><rect x=\"0\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#008fd5;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"55\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#fc4f30;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"110\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#e5ae38;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"165\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#6d904f;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"220\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#8b8b8b;stroke-width:2;stroke:rgb(255,255,255)\"/><rect x=\"275\" y=\"0\" width=\"55\" height=\"55\" style=\"fill:#810f7c;stroke-width:2;stroke:rgb(255,255,255)\"/></svg>"
      ],
      "text/plain": [
       "[(0.0, 0.5607843137254902, 0.8352941176470589),\n",
       " (0.9882352941176471, 0.30980392156862746, 0.18823529411764706),\n",
       " (0.8980392156862745, 0.6823529411764706, 0.2196078431372549),\n",
       " (0.42745098039215684, 0.5647058823529412, 0.30980392156862746),\n",
       " (0.5450980392156862, 0.5450980392156862, 0.5450980392156862),\n",
       " (0.5058823529411764, 0.058823529411764705, 0.48627450980392156)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "color_pal = sns.color_palette()\n",
    "color_pal\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Function Definition**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Data Cleaning**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_columns_for_ml(df):\n",
    "    for col in df.columns:\n",
    "        # ---------------------------------\n",
    "        if 'WK_NBR_IN_YEAR' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if 'WK_NBR_IN_HALF' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if 'WK_NBR_IN_QTR' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if 'WK_NBR_IN_PRD' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "        # ---------------------------------\n",
    "        if 'FSCL_YR' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if 'FSCL_HALF_NBR' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if 'FSCL_QTR_NBR' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if 'FSCL_PRD_NBR' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "        # ---------------------------------\n",
    "        if 'DPT_NBR' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if 'CLS_NBR' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if 'MVNDR_NBR' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "        # ---------------------------------\n",
    "        if 'PNL_IND' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_columns(df):\n",
    "    for col in df.columns:\n",
    "        # ---------------------------------\n",
    "        if 'SNSH_YR_WK' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'FSCL_YR_WK' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'FSCL_YR_WK_KEY_VAL' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'FSCL_YR_PRD' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'FSCL_PRD_KEY_VAL' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'FSCL_YR_QTR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "        if 'WK_NBR_IN_YEAR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'WK_NBR_IN_HALF' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'WK_NBR_IN_QTR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'WK_NBR_IN_PRD' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "        if 'FSCL_YR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'FSCL_HALF_NBR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'FSCL_QTR_NBR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'FSCL_PRD_NBR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "        # ---------------------------------\n",
    "        if 'PNL_IND' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "        # ---------------------------------\n",
    "        if 'DPT_NBR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'CLS_NBR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'MVNDR_NBR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "        # ---------------------------------\n",
    "        if 'MOT_ID' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'SVC_LVL_ID' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'SHP_TYP_CD' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "        # ---------------------------------\n",
    "        if 'LOC_ID' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'LOC_ALS_ID' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'ALLOC_LOC_NBR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'DC_NBR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'ORIG_LOC_NBR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'DEST_LOC_NBR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "        # ---------------------------------\n",
    "        if 'COST' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if 'RATE' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if 'AMT' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "        # ---------------------------------\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_null(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype.name == 'category':\n",
    "            df[col] = df[col].replace([-1, 0, 'UNK', 'NULL', 'NaN'], np.nan)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Query Data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query data here\n",
    "    # consider bringing in prebuilt sklearn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# stop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Clean Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[['WK_BGN_DT','FLOW_OB_STR_AMT']]\n",
    "df = convert_columns(df)\n",
    "df = replace_with_null(df)\n",
    "df['WK_BGN_DT'] = pd.to_datetime(df['WK_BGN_DT'])\n",
    "# df = df.sort_values('WK_BGN_DT', ascending=False)\n",
    "df = df.set_index('WK_BGN_DT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into actuals and future dataframes\n",
    "df_fcst = df.query('IS_FCST == True').copy()\n",
    "df = df.query('IS_FCST == False').copy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Profiling (Pre-modeling)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[39m.\u001b[39minfo()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FLOW_OB_STR_AMT'].plot(\n",
    "    style='.',\n",
    "    figsize=(15, 5),\n",
    "    color=color_pal[0],\n",
    "    title='Intl Store Landed Flow',\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train / Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_split_date = '07-01-2022'  # variable of split date\n",
    "\n",
    "df_train = df.loc[df.index < test_train_split_date]\n",
    "df_test = df.loc[df.index >= test_train_split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "df_train['FLOW_OB_STR_AMT'].plot(\n",
    "    ax=ax, style='.', label='Training Set', title='Train/Test Split')\n",
    "df_test['FLOW_OB_STR_AMT'].plot(ax=ax, style='.', label='Test Set')\n",
    "ax.axvline(test_train_split_date, color='black', ls='--')\n",
    "ax.legend(['Training Set', 'Test Set'])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Forecasting Horizon**\n",
    "\n",
    "<br>\n",
    "\n",
    "- The forecast horizon is the length of time into the future for which forecasts are to be prepared. These generally vary from short-term forecasting horizons (less than 3 months) to long-term horizons (more than two years).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Visualize Feature / Target Relationship**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "\n",
    "ax.set_title = 'OB_FLOW by week in half'\n",
    "sns.boxplot(\n",
    "    data=df,\n",
    "    x='WK_NBR_IN_HALF',\n",
    "    y='FLOW_OB_STR_AMT',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Experiments with creating test models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features for training here\n",
    "FEATURES = [\n",
    "    'FSCL_YR',\n",
    "    'FSCL_HALF_NBR',\n",
    "    'FSCL_QTR_NBR',\n",
    "    'FSCL_PRD_NBR',\n",
    "    'WK_NBR_IN_YEAR',\n",
    "    'WK_NBR_IN_HALF',\n",
    "    'WK_NBR_IN_QTR',\n",
    "    'WK_NBR_IN_PRD',\n",
    "    'PNL_IND',\n",
    "    'MERCH_DPT_NBR',\n",
    "    'MERCH_CLS_NBR',\n",
    "    'lag1',\n",
    "    'lag2',\n",
    "    'lag3',\n",
    "    'lag4',\n",
    "]\n",
    "\n",
    "# Define target features here\n",
    "TARGET = 'FLOW_OB_STR_AMT'\n",
    "\n",
    "# %%\n",
    "df_train = convert_columns_for_ml(df_train)\n",
    "df_test = convert_columns_for_ml(df_test)\n",
    "\n",
    "# %%\n",
    "X_train = df_train[FEATURES]\n",
    "y_train = df_train[TARGET]\n",
    "\n",
    "X_test = df_test[FEATURES]\n",
    "y_test = df_test[TARGET]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hyperparameter Optimization WIP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocessing for hyperparameter tuning\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "# y_train = le.fit_transform(y_train)\n",
    "\n",
    "# %%\n",
    "# # import packages for hyperparameters tuning\n",
    "# from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# #initialize the domain space for hyperparameters to be optimized\n",
    "# space={\n",
    "#         'n_estimators': 100,\n",
    "#         'max_depth': hp.quniform('max_depth', 3, 12, 1),\n",
    "#         # 'gamma': hp.uniform ('gamma', 0.2,5),\n",
    "#         # 'reg_alpha' : hp.quniform('reg_alpha', 0.001, 0.1, 1),\n",
    "#         # 'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "#         # 'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "#         # 'min_child_weight' : hp.quniform('min_child_weight', 1, 8, 1),\n",
    "#         'seed': 0,\n",
    "#     }\n",
    "\n",
    "# def objective(space):\n",
    "#     clf=xgb.XGBClassifier(\n",
    "#                     n_estimators =space['n_estimators'],\n",
    "#                     max_depth = int(space['max_depth']),\n",
    "#                     # gamma = space['gamma'],\n",
    "#                     # reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),\n",
    "#                     # colsample_bytree=int(space['colsample_bytree'])\n",
    "#                     )\n",
    "\n",
    "#     evaluation = [( X_train, y_train), ( X_test, y_test)]\n",
    "\n",
    "#     clf.fit(X_train, y_train,\n",
    "#             eval_set=evaluation, eval_metric=\"auc\",\n",
    "#             early_stopping_rounds=10,verbose=False)\n",
    "\n",
    "\n",
    "#     pred = clf.predict(X_test)\n",
    "#     accuracy = accuracy_score(y_test, pred>0.5)\n",
    "#     print (\"SCORE:\", accuracy)\n",
    "#     return {'loss': -accuracy, 'status': STATUS_OK }\n",
    "\n",
    "# %%\n",
    "# trials = Trials()\n",
    "\n",
    "# best_hyperparams = fmin(fn = objective,\n",
    "#                         space = space,\n",
    "#                         algo = tpe.suggest,\n",
    "#                         max_evals = 100,\n",
    "#                         trials = trials)\n",
    "\n",
    "# %%\n",
    "# print(\"The best hyperparameters are : \",\"\\n\")\n",
    "# print(best_hyperparams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train/Test Proof**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final, optimized hyper parameters for training final model here\n",
    "reg = xgb.XGBRegressor(\n",
    "    booster='gbtree',\n",
    "    objective='reg:squarederror',\n",
    "    base_score=0.5,\n",
    "    n_estimators=1500,  # tuned\n",
    "    min_child_weight=3,\n",
    "    gamma=0,\n",
    "    learning_rate=0.048,  # tuned\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    max_depth=12,\n",
    ")\n",
    "\n",
    "reg.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    verbose=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our model is now trained**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi = pd.DataFrame(\n",
    "    data=reg.feature_importances_,\n",
    "    index=reg.feature_names_in_,\n",
    "    columns=['importance']\n",
    ")\n",
    "df_fi.sort_values('importance').plot(kind='barh', title='Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test forecast on the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['PREDICTION'] = reg.predict(X_test)\n",
    "df_test['PREDICTION'] = df_test['PREDICTION'].apply(\n",
    "    lambda x: 0 if x < 0 else x)  # controlling for negative predictions\n",
    "cols = [\n",
    "    'WK_BGN_DT',\n",
    "    'FSCL_YR_WK',\n",
    "    'PNL_IND',\n",
    "    'MERCH_DPT_NBR',\n",
    "    'MERCH_CLS_NBR',\n",
    "    'PREDICTION',\n",
    "]\n",
    "df_test = df_test[cols]\n",
    "df_test = convert_columns(df_test)\n",
    "df_test = df_test.set_index('WK_BGN_DT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.merge(\n",
    "    df,\n",
    "    df_test,\n",
    "    how='left',\n",
    "    on=[\n",
    "        'WK_BGN_DT',\n",
    "        'FSCL_YR_WK',\n",
    "        'PNL_IND',\n",
    "        'MERCH_DPT_NBR',\n",
    "        'MERCH_CLS_NBR',\n",
    "    ],\n",
    ")\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_test[['FLOW_OB_STR_AMT']].plot(figsize=(20, 10), style='.')\n",
    "df_test['PREDICTION'].plot(\n",
    "    ax=ax,\n",
    "    style='.'\n",
    ")\n",
    "plt.legend(['Truth Data', 'Predictions'])\n",
    "ax.set_title('Raw data and predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Calculate root mean squared error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.loc[df_test.index >= test_train_split_date]\n",
    "score = np.sqrt(mean_squared_error(\n",
    "    df_test['FLOW_OB_STR_AMT'], df_test['PREDICTION']))\n",
    "stdev = df_test['FLOW_OB_STR_AMT'].std()\n",
    "normalized_score = score / stdev\n",
    "print(f'RMSE Score on Test set: {score:0.6f}')\n",
    "print(f'RMSE / StdDev on Test set: {normalized_score:0.6f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Calculate Nominal Error**\n",
    "\n",
    "- Look at the worst and best predicted weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['abs_error'] = np.abs(df_test[TARGET] - df_test['PREDICTION'])\n",
    "df_test['error'] = df_test[TARGET] - df_test['PREDICTION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.to_clipboard(excel=True, index=False, header=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Predicting into the future**\n",
    "\n",
    "- Retraining on all data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features for training here\n",
    "FEATURES = [\n",
    "    'FSCL_YR',\n",
    "    'FSCL_HALF_NBR',\n",
    "    'FSCL_QTR_NBR',\n",
    "    'FSCL_PRD_NBR',\n",
    "    'WK_NBR_IN_YEAR',\n",
    "    'WK_NBR_IN_HALF',\n",
    "    'WK_NBR_IN_QTR',\n",
    "    'WK_NBR_IN_PRD',\n",
    "    'PNL_IND',\n",
    "    'MERCH_DPT_NBR',\n",
    "    'MERCH_CLS_NBR',\n",
    "    'lag1',\n",
    "    'lag2',\n",
    "    'lag3',\n",
    "    'lag4',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target features here\n",
    "TARGET = 'FLOW_OB_STR_AMT'\n",
    "\n",
    "x_all = df[FEATURES]\n",
    "y_all = df[TARGET]\n",
    "\n",
    "reg = xgb.XGBRegressor(\n",
    "    booster='gbtree',\n",
    "    # objective = 'reg:linear',\n",
    "    objective='reg:squarederror',\n",
    "    base_score=0.5,\n",
    "    n_estimators=1500,  # tuned\n",
    "    # n_estimators = 1000,\n",
    "    learning_rate=0.042,  # tuned\n",
    "    # learning_rate = 0.042,\n",
    "    max_depth=12,  # tuned\n",
    "    # max_depth = 6,\n",
    ")\n",
    "reg.fit(\n",
    "    x_all,\n",
    "    y_all,\n",
    "    eval_set=[(x_all, y_all)],\n",
    "    verbose=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Predict the future**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fcst = convert_columns_for_ml(df_fcst)\n",
    "df_fcst['PREDICTION'] = reg.predict(df_fcst[FEATURES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the future\n",
    "df_fcst['PREDICTION'].plot(\n",
    "    figsize=(10, 5),\n",
    "    style='.',\n",
    "    color=color_pal[5],\n",
    "    ms=1,\n",
    "    lw=1, title='Future Predictions'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Saving Model for Later**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.save_model('model.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading it back up for validation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_new = xgb.XGBRegressor()\n",
    "reg_new.load_model('model.json')\n",
    "# predict the future\n",
    "df_fcst['PREDICTION'] = reg.predict(df_fcst[FEATURES])\n",
    "# plot the future\n",
    "df_fcst['PREDICTION'].plot(\n",
    "    figsize=(10, 5),\n",
    "    style='.',\n",
    "    color=color_pal[5],\n",
    "    ms=1,\n",
    "    lw=1, title='Future Predictions'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fcst.to_clipboard(excel=True, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "bf2256466664d08ad5829690b78a52ff85021f5c7c81ebd85ac567841dd5c95f"
   }
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "d3e10ef16274dd72e574b8fa73b58450b957d8421a2901baded3cca26fcf5dda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
