{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **xgboost_timeseries_modeling_template**\n",
    "\n",
    "---\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Environment Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# custom files\n",
    "from fp_data_toolbox import eda, notifier # general eda functions and win toast notifier on cell completion\n",
    "notifier.setup() #Enable for windows toast notifications on Jupyter cell complete\n",
    "# Magics env settings...\n",
    "%matplotlib inline\n",
    "# env variables\n",
    "df = pd.DataFrame() # creating empty dataframe variable\n",
    "params = {} # creating empty parameters dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Variable Inputs**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_name = ''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Objectives**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of dataset and/or problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualizing ETL pipeline (example)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# custom imports\n",
    "# general eda functions and win toast notifier on cell completion\n",
    "from fp_data_toolbox import eda, notifier, environment\n",
    "# Magics env settings...\n",
    "# Setup sqlalchemy connection url for MSSQL Server connection here\n",
    "%matplotlib inline\n",
    "%load_ext google.cloud.bigquery\n",
    "%load_ext sql\n",
    "%env DATABASE_URL = mssql+pyodbc: // SCFDW2/scfdw_core?driver = SQL+Server+Native+Client+11.0\n",
    "%config SqlMagic.autocommit = True\n",
    "%config SqlMagic.autopandas = True\n",
    "# env setup functions\n",
    "notifier.setup()  # Enable for windows toast notifications on Jupyter cell complete\n",
    "# Enable to setup a ydata_profiling config.yaml file in the parent project\n",
    "yaml_config_path = environment.ydata_yaml_setup()\n",
    "# env variables\n",
    "gbq_project_id = 'analytics-scfinance-thd'  # USER INPUT\n",
    "sql_conn = 'mssql+pyodbc://SCFDW2/scfdw_core?driver=SQL+Server+Native+Client+11.0'  # USER INPUT\n",
    "df = pd.DataFrame()    # creating empty dataframe variable\n",
    "params = {}            # creating empty parameters dictionary\n",
    "# params = fcal.pull_fin_cal_temp_var()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost_explore_test\n",
    "# ---\n",
    "#\n",
    "# <br><br><br><br>\n",
    "\n",
    "# %% [markdown]\n",
    "# ## **Environment Setup**\n",
    "# ---\n",
    "\n",
    "# %% [markdown]\n",
    "# ### **Imports and Settings**\n",
    "\n",
    "# %%\n",
    "# imports\n",
    "\n",
    "# %%\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "color_pal = sns.color_palette()\n",
    "color_pal\n",
    "\n",
    "# %% [markdown]\n",
    "# <br><br><br>\n",
    "\n",
    "# %% [markdown]\n",
    "# ## **Function Definition**\n",
    "\n",
    "# %% [markdown]\n",
    "# ### **Data Cleaning**\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "def convert_columns_for_ml(df):\n",
    "    for col in df.columns:\n",
    "        # ---------------------------------\n",
    "        if 'WK_NBR_IN_YEAR' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if 'WK_NBR_IN_HALF' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if 'WK_NBR_IN_QTR' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if 'WK_NBR_IN_PRD' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "        # ---------------------------------\n",
    "        if 'FSCL_YR' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if 'FSCL_HALF_NBR' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if 'FSCL_QTR_NBR' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if 'FSCL_PRD_NBR' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "        # ---------------------------------\n",
    "        if 'DPT_NBR' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if 'CLS_NBR' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if 'MVNDR_NBR' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "        # ---------------------------------\n",
    "        if 'PNL_IND' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_columns(df):\n",
    "    for col in df.columns:\n",
    "        # ---------------------------------\n",
    "        if 'SNSH_YR_WK' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'FSCL_YR_WK' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'FSCL_YR_WK_KEY_VAL' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'FSCL_YR_PRD' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'FSCL_PRD_KEY_VAL' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'FSCL_YR_QTR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "        if 'WK_NBR_IN_YEAR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'WK_NBR_IN_HALF' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'WK_NBR_IN_QTR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'WK_NBR_IN_PRD' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "        if 'FSCL_YR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'FSCL_HALF_NBR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'FSCL_QTR_NBR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'FSCL_PRD_NBR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "        # ---------------------------------\n",
    "        if 'PNL_IND' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "        # ---------------------------------\n",
    "        if 'DPT_NBR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'CLS_NBR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'MVNDR_NBR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "        # ---------------------------------\n",
    "        if 'MOT_ID' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'SVC_LVL_ID' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'SHP_TYP_CD' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "        # ---------------------------------\n",
    "        if 'LOC_ID' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'LOC_ALS_ID' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'ALLOC_LOC_NBR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'DC_NBR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'ORIG_LOC_NBR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "        if 'DEST_LOC_NBR' in col:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "        # ---------------------------------\n",
    "        if 'COST' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if 'RATE' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if 'AMT' in col:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "        # ---------------------------------\n",
    "    return df\n",
    "\n",
    "\n",
    "def replace_with_null(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype.name == 'category':\n",
    "            df[col] = df[col].replace([-1, 0, 'UNK', 'NULL', 'NaN'], np.nan)\n",
    "    return df\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# <br><br><br><br><br>\n",
    "\n",
    "# %% [markdown]\n",
    "# ## **Query Data**\n",
    "\n",
    "# %%\n",
    "% % bigquery df\n",
    "# qry='''\n",
    "# %%\n",
    "# df.to_clipboard(excel=True, index=False, header=True)\n",
    "\n",
    "# %%\n",
    "len(df)\n",
    "\n",
    "# %%\n",
    "# stop\n",
    "\n",
    "# %% [markdown]\n",
    "# <br><br><br><br>\n",
    "#\n",
    "# ## **Clean Data**\n",
    "\n",
    "# %%\n",
    "\n",
    "# df = df[['WK_BGN_DT','FLOW_OB_STR_AMT']]\n",
    "df = convert_columns(df)\n",
    "df = replace_with_null(df)\n",
    "df['WK_BGN_DT'] = pd.to_datetime(df['WK_BGN_DT'])\n",
    "# df = df.sort_values('WK_BGN_DT', ascending=False)\n",
    "df = df.set_index('WK_BGN_DT')\n",
    "\n",
    "# %%\n",
    "# split into actuals and future dataframes\n",
    "df_fcst = df.query('IS_FCST == True').copy()\n",
    "df = df.query('IS_FCST == False').copy()\n",
    "\n",
    "# %% [markdown]\n",
    "# <br><br><br>\n",
    "#\n",
    "# ## **Data Profiling (Pre-modeling)**\n",
    "\n",
    "# %%\n",
    "df.info()\n",
    "\n",
    "# %%\n",
    "df.head(5)\n",
    "\n",
    "# %%\n",
    "# stop\n",
    "\n",
    "# %%\n",
    "df['FLOW_OB_STR_AMT'].plot(\n",
    "    style='.',\n",
    "    figsize=(15, 5),\n",
    "    color=color_pal[0],\n",
    "    title='Intl Store Landed Flow',\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# stop\n",
    "\n",
    "# %% [markdown]\n",
    "# <br><br><br>\n",
    "#\n",
    "#\n",
    "# ## **Train / Test Split**\n",
    "\n",
    "# %%\n",
    "test_train_split_date = '07-01-2022'  # variable of split date\n",
    "\n",
    "df_train = df.loc[df.index < test_train_split_date]\n",
    "df_test = df.loc[df.index >= test_train_split_date]\n",
    "\n",
    "# %%\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "df_train['FLOW_OB_STR_AMT'].plot(\n",
    "    ax=ax, style='.', label='Training Set', title='Train/Test Split')\n",
    "df_test['FLOW_OB_STR_AMT'].plot(ax=ax, style='.', label='Test Set')\n",
    "ax.axvline(test_train_split_date, color='black', ls='--')\n",
    "ax.legend(['Training Set', 'Test Set'])\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# <br><br><br>\n",
    "#\n",
    "# ## **Forecasting Horizon**\n",
    "#\n",
    "# - The forecast horizon is the length of time into the future for which forecasts are to be prepared. These generally vary from short-term forecasting horizons (less than 3 months) to long-term horizons (more than two years).\n",
    "#\n",
    "\n",
    "# %% [markdown]\n",
    "# <br><br><br>\n",
    "#\n",
    "# ## **Visualize Feature / Target Relationship**\n",
    "\n",
    "# %%\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "\n",
    "ax.set_title = 'OB_FLOW by week in half'\n",
    "sns.boxplot(\n",
    "    data=df,\n",
    "    x='WK_NBR_IN_HALF',\n",
    "    y='FLOW_OB_STR_AMT',\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# <br><br><br>\n",
    "#\n",
    "# ## **Experiments with creating test models**\n",
    "\n",
    "# %%\n",
    "# Define features for training here\n",
    "FEATURES = [\n",
    "    'FSCL_YR',\n",
    "    'FSCL_HALF_NBR',\n",
    "    'FSCL_QTR_NBR',\n",
    "    'FSCL_PRD_NBR',\n",
    "    'WK_NBR_IN_YEAR',\n",
    "    'WK_NBR_IN_HALF',\n",
    "    'WK_NBR_IN_QTR',\n",
    "    'WK_NBR_IN_PRD',\n",
    "    'PNL_IND',\n",
    "    'MERCH_DPT_NBR',\n",
    "    'MERCH_CLS_NBR',\n",
    "    'lag1',\n",
    "    'lag2',\n",
    "    'lag3',\n",
    "    'lag4',\n",
    "]\n",
    "\n",
    "# Define target features here\n",
    "TARGET = 'FLOW_OB_STR_AMT'\n",
    "\n",
    "# %%\n",
    "df_train = convert_columns_for_ml(df_train)\n",
    "df_test = convert_columns_for_ml(df_test)\n",
    "\n",
    "# %%\n",
    "X_train = df_train[FEATURES]\n",
    "y_train = df_train[TARGET]\n",
    "\n",
    "X_test = df_test[FEATURES]\n",
    "y_test = df_test[TARGET]\n",
    "\n",
    "# %% [markdown]\n",
    "# <br><br><br>\n",
    "#\n",
    "#\n",
    "# ### **Hyperparameter Optimization WIP**\n",
    "\n",
    "# %%\n",
    "# # preprocessing for hyperparameter tuning\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "# y_train = le.fit_transform(y_train)\n",
    "\n",
    "# %%\n",
    "# # import packages for hyperparameters tuning\n",
    "# from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# #initialize the domain space for hyperparameters to be optimized\n",
    "# space={\n",
    "#         'n_estimators': 100,\n",
    "#         'max_depth': hp.quniform('max_depth', 3, 12, 1),\n",
    "#         # 'gamma': hp.uniform ('gamma', 0.2,5),\n",
    "#         # 'reg_alpha' : hp.quniform('reg_alpha', 0.001, 0.1, 1),\n",
    "#         # 'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "#         # 'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "#         # 'min_child_weight' : hp.quniform('min_child_weight', 1, 8, 1),\n",
    "#         'seed': 0,\n",
    "#     }\n",
    "\n",
    "# def objective(space):\n",
    "#     clf=xgb.XGBClassifier(\n",
    "#                     n_estimators =space['n_estimators'],\n",
    "#                     max_depth = int(space['max_depth']),\n",
    "#                     # gamma = space['gamma'],\n",
    "#                     # reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),\n",
    "#                     # colsample_bytree=int(space['colsample_bytree'])\n",
    "#                     )\n",
    "\n",
    "#     evaluation = [( X_train, y_train), ( X_test, y_test)]\n",
    "\n",
    "#     clf.fit(X_train, y_train,\n",
    "#             eval_set=evaluation, eval_metric=\"auc\",\n",
    "#             early_stopping_rounds=10,verbose=False)\n",
    "\n",
    "\n",
    "#     pred = clf.predict(X_test)\n",
    "#     accuracy = accuracy_score(y_test, pred>0.5)\n",
    "#     print (\"SCORE:\", accuracy)\n",
    "#     return {'loss': -accuracy, 'status': STATUS_OK }\n",
    "\n",
    "# %%\n",
    "# trials = Trials()\n",
    "\n",
    "# best_hyperparams = fmin(fn = objective,\n",
    "#                         space = space,\n",
    "#                         algo = tpe.suggest,\n",
    "#                         max_evals = 100,\n",
    "#                         trials = trials)\n",
    "\n",
    "# %%\n",
    "# print(\"The best hyperparameters are : \",\"\\n\")\n",
    "# print(best_hyperparams)\n",
    "\n",
    "# %%\n",
    "# stop\n",
    "\n",
    "# %% [markdown]\n",
    "# <br><br><br>\n",
    "#\n",
    "\n",
    "# %% [markdown]\n",
    "# ### **Train/Test Proof**\n",
    "\n",
    "# %%\n",
    "# Save final, optimized hyper parameters for training final model here\n",
    "reg = xgb.XGBRegressor(\n",
    "    booster='gbtree',\n",
    "    objective='reg:squarederror',\n",
    "    base_score=0.5,\n",
    "    n_estimators=1500,  # tuned\n",
    "    min_child_weight=3,\n",
    "    gamma=0,\n",
    "    learning_rate=0.048,  # tuned\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    max_depth=12,\n",
    ")\n",
    "\n",
    "\n",
    "# %%\n",
    "reg.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    verbose=100,\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# **Our model is now trained**\n",
    "\n",
    "# %% [markdown]\n",
    "# <br><br><br>\n",
    "#\n",
    "# ## **Feature Importance**\n",
    "\n",
    "# %%\n",
    "df_fi = pd.DataFrame(\n",
    "    data=reg.feature_importances_,\n",
    "    index=reg.feature_names_in_,\n",
    "    columns=['importance']\n",
    ")\n",
    "df_fi.sort_values('importance').plot(kind='barh', title='Feature Importance')\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# <br><br><br>\n",
    "#\n",
    "# ## **Test forecast on the test set**\n",
    "\n",
    "# %%\n",
    "df_test['PREDICTION'] = reg.predict(X_test)\n",
    "df_test['PREDICTION'] = df_test['PREDICTION'].apply(\n",
    "    lambda x: 0 if x < 0 else x)  # controlling for negative predictions\n",
    "cols = [\n",
    "    'WK_BGN_DT',\n",
    "    'FSCL_YR_WK',\n",
    "    'PNL_IND',\n",
    "    'MERCH_DPT_NBR',\n",
    "    'MERCH_CLS_NBR',\n",
    "    'PREDICTION',\n",
    "]\n",
    "df_test = df_test[cols]\n",
    "df_test = convert_columns(df_test)\n",
    "df_test = df_test.set_index('WK_BGN_DT')\n",
    "\n",
    "# %%\n",
    "df_test = pd.merge(\n",
    "    df,\n",
    "    df_test,\n",
    "    how='left',\n",
    "    on=[\n",
    "        'WK_BGN_DT',\n",
    "        'FSCL_YR_WK',\n",
    "        'PNL_IND',\n",
    "        'MERCH_DPT_NBR',\n",
    "        'MERCH_CLS_NBR',\n",
    "    ],\n",
    ")\n",
    "df_test.info()\n",
    "\n",
    "# %%\n",
    "ax = df_test[['FLOW_OB_STR_AMT']].plot(figsize=(20, 10), style='.')\n",
    "df_test['PREDICTION'].plot(\n",
    "    ax=ax,\n",
    "    style='.'\n",
    ")\n",
    "plt.legend(['Truth Data', 'Predictions'])\n",
    "ax.set_title('Raw data and predictions')\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# <br><br><br>\n",
    "#\n",
    "# ## **Calculate root mean squared error**\n",
    "\n",
    "# %%\n",
    "\n",
    "df_test = df_test.loc[df_test.index >= test_train_split_date]\n",
    "score = np.sqrt(mean_squared_error(\n",
    "    df_test['FLOW_OB_STR_AMT'], df_test['PREDICTION']))\n",
    "stdev = df_test['FLOW_OB_STR_AMT'].std()\n",
    "normalized_score = score / stdev\n",
    "print(f'RMSE Score on Test set: {score:0.6f}')\n",
    "print(f'RMSE / StdDev on Test set: {normalized_score:0.6f}')\n",
    "\n",
    "# %% [markdown]\n",
    "# <br><br><br>\n",
    "#\n",
    "# ## **Calculate Nominal Error**\n",
    "#\n",
    "# - Look at the worst and best predicted weeks\n",
    "\n",
    "# %%\n",
    "df_test['abs_error'] = np.abs(df_test[TARGET] - df_test['PREDICTION'])\n",
    "df_test['error'] = df_test[TARGET] - df_test['PREDICTION']\n",
    "\n",
    "# %%\n",
    "# df_test.to_clipboard(excel=True, index=False, header=True)\n",
    "\n",
    "# %%\n",
    "# stop\n",
    "\n",
    "# %% [markdown]\n",
    "# <br><br><br>\n",
    "#\n",
    "# ## **Predicting into the future**\n",
    "#\n",
    "# - Retraining on all data\n",
    "\n",
    "# %%\n",
    "# Define features for training here\n",
    "FEATURES = [\n",
    "    'FSCL_YR',\n",
    "    'FSCL_HALF_NBR',\n",
    "    'FSCL_QTR_NBR',\n",
    "    'FSCL_PRD_NBR',\n",
    "    'WK_NBR_IN_YEAR',\n",
    "    'WK_NBR_IN_HALF',\n",
    "    'WK_NBR_IN_QTR',\n",
    "    'WK_NBR_IN_PRD',\n",
    "    'PNL_IND',\n",
    "    'MERCH_DPT_NBR',\n",
    "    'MERCH_CLS_NBR',\n",
    "    'lag1',\n",
    "    'lag2',\n",
    "    'lag3',\n",
    "    'lag4',\n",
    "]\n",
    "\n",
    "# Define target features here\n",
    "TARGET = 'FLOW_OB_STR_AMT'\n",
    "\n",
    "x_all = df[FEATURES]\n",
    "y_all = df[TARGET]\n",
    "\n",
    "reg = xgb.XGBRegressor(\n",
    "    booster='gbtree',\n",
    "    # objective = 'reg:linear',\n",
    "    objective='reg:squarederror',\n",
    "    base_score=0.5,\n",
    "    n_estimators=1500,  # tuned\n",
    "    # n_estimators = 1000,\n",
    "    learning_rate=0.042,  # tuned\n",
    "    # learning_rate = 0.042,\n",
    "    max_depth=12,  # tuned\n",
    "    # max_depth = 6,\n",
    ")\n",
    "reg.fit(\n",
    "    x_all,\n",
    "    y_all,\n",
    "    eval_set=[(x_all, y_all)],\n",
    "    verbose=50,\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### **Predict the future**\n",
    "\n",
    "# %%\n",
    "df_fcst = convert_columns_for_ml(df_fcst)\n",
    "df_fcst['PREDICTION'] = reg.predict(df_fcst[FEATURES])\n",
    "\n",
    "# %%\n",
    "# plot the future\n",
    "df_fcst['PREDICTION'].plot(\n",
    "    figsize=(10, 5),\n",
    "    style='.',\n",
    "    color=color_pal[5],\n",
    "    ms=1,\n",
    "    lw=1, title='Future Predictions'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# <br><br><br>\n",
    "#\n",
    "# ## **Saving Model for Later**\n",
    "\n",
    "# %%\n",
    "reg.save_model('model.json')\n",
    "\n",
    "# %% [markdown]\n",
    "# ### **Loading it back up for validation**\n",
    "\n",
    "# %%\n",
    "reg_new = xgb.XGBRegressor()\n",
    "reg_new.load_model('model.json')\n",
    "# predict the future\n",
    "df_fcst['PREDICTION'] = reg.predict(df_fcst[FEATURES])\n",
    "# plot the future\n",
    "df_fcst['PREDICTION'].plot(\n",
    "    figsize=(10, 5),\n",
    "    style='.',\n",
    "    color=color_pal[5],\n",
    "    ms=1,\n",
    "    lw=1, title='Future Predictions'\n",
    ")\n",
    "\n",
    "# %%\n",
    "df_fcst.to_clipboard(excel=True, index=False, header=True)\n",
    "\n",
    "# %%\n",
    "# stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imports / Environment Setup**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports and Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# --- custom files\n",
    "import project_path # .py file that includes the project path for module imports\n",
    "from fp_data_toolbox import eda, notifier # general eda functions and win toast notifier on cell completion\n",
    "notifier.setup() #Enable for windows toast notifications on Jupyter cell complete\n",
    "# --- Magics env settings...\n",
    "%matplotlib inline\n",
    "# --- env variables\n",
    "df = pd.DataFrame() # creating empty dataframe variable\n",
    "params = {} # creating empty parameters dictionary\n",
    "# --- connection variables setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Variable setup**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = ''\n",
    "input_dir = ''\n",
    "input_path = input_dir+input_file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ingestion/Preprocessing**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_feather('C:\\\\git\\\\fp_data_toolbox-proj\\\\data\\\\examples\\\\oracle_cards-2022-10-25.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_df = df.loc[df['concat_index_colm'].isna()] # select where concat_index_colm in NaN (some part of your concat is null)\n",
    "#eda.copi_df(nan_df)\n",
    "# nan_df # select where concat_index_colm in NaN (some part of your concat is null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dupe_df = df.loc[df['concat_index_colm'] == '']\n",
    "# eda.copi_df(dupe_df)\n",
    "# dupe_df # example duplicate record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Finding PK/Index of view**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we've chosen the correct index/primary key combination, the below cell will NOT return an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.set_index('concat_index_colm'\n",
    "#                 ,verify_integrity=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dataset failed the test, single out the record with the highest frequency and copi it for analysis in excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Profile Reports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profile = df.iloc[:100, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.pandas_profiling_min_nb_frame(df_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.dataprep_rprt_show(df_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keys:\n",
    "\n",
    "- df name: df_example\n",
    "    - PKs\n",
    "\n",
    "<br>\n",
    "\n",
    "- df name: df_example\n",
    "    - PKs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Cleaning**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date fields to datetime dtype (for better pandas based analysis)\n",
    "# eda.cast_as_datetime(df,'shp_dt');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - fill this in later as I work on more projects\n",
    "# TODO - At some point, transition the data cleaning operations to their own functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning operations here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Formatting**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - fill this in later as I work on more projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data formatting operations here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Processing**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - fill this in later as I work on more projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing operations here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Outputs**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final output operations here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "bf2256466664d08ad5829690b78a52ff85021f5c7c81ebd85ac567841dd5c95f"
   }
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "d3e10ef16274dd72e574b8fa73b58450b957d8421a2901baded3cca26fcf5dda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
